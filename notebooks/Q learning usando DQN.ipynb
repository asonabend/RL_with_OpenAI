{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando Cartpole con Reinforcement Learning usando Deep Q-learning\n",
    "\n",
    "Este cuaderno es una modificación del tutorial de [Pytorch RL DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    " \n",
    "Sigue la línea de clase de Reinforcement Learning, Q-learning & OpenAI de la RIIA 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Veamos de qué se trata el ambiente de Cartpole:\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(30):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # Toma acción aleatoria\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias:\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "# Las soluciones usan pytorch, pueden usar keras y/o tensorflow si prefieren\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Ambiente de OpenAI \"Cart pole\"\n",
    "enviroment = gym.make('CartPole-v0').unwrapped\n",
    "enviroment.render()\n",
    "\n",
    "# Revisa si hay GPU disponible y lo utiliza\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Número de acciones: {}'.format(enviroment.action_space.n))\n",
    "print('Dimensión de estado: {}'.format(enviroment.observation_space))\n",
    "\n",
    "# Factor de descuento temporal\n",
    "gamma = \n",
    "# Número de muestras que extraer del repositorio de experiencia para entrenar la red\n",
    "No_grupo = \n",
    "# Parámetros para la tasa de epsilon-gredy, ésta va cayendo exponencialmente\n",
    "eps_inicial = \n",
    "eps_final = \n",
    "eps_tasa = \n",
    "# Parámetro para el descenso por gradiente estocástico\n",
    "lr = \n",
    "# Cada cuanto actualizar la red de etiqueta\n",
    "actualizar_red_med =\n",
    "# Número de episodios para entrenar\n",
    "No_episodios = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define una función llamda `genera_accion` que reciba el vector del `estado` y tome la acción óptima o una acción aleatoria. La acción aleatoria la debe de tomar con una probabilidad que disminuya exponencialmente, de tal manera que en un principio se explore más.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con probabilidad $$\\epsilon_{final}+(\\epsilon_{inicial}-\\epsilon_{final})\\times e^{-iters/tasa_{\\epsilon}}$$ se escoge una acción aleatoria. En la siguiente gráfica se puede observar la tasa que cae exponencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([eps_final + (eps_inicial - eps_final) * math.exp(-1. * iters / eps_tasa) for iters in range(1000)])\n",
    "plt.title('Disminución exponencial de la tasa de exploración')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Probabilidad de explorar: $\\epsilon$')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_accion(estado):\n",
    "    global iters\n",
    "    decimal = random.uniform(0, 1) \n",
    "    limite_epsilon = eps_final + (eps_inicial - eps_final) * math.exp(-1. * iters / eps_tasa)\n",
    "    iters += 1\n",
    "    if decimal > limite_epsilon:\n",
    "        with torch.no_grad():\n",
    "            return red_estrategia(estado).max(0)[1].view(1)\n",
    "    else:\n",
    "        return torch.tensor([random.randrange(2)], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera una red neuronal que reciba el vector de estado y regrese un vector de dimensión igual al número de acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class red_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(red_N, self).__init__()\n",
    "        # Capas densas\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Arquitectura de la red, con activación ReLU en las dos capas interiores\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En a siguiente celda generamos una clase de repositorio de experiencia con diferentes atributos:\n",
    "\n",
    "\n",
    "`guarda`: guarda la observación $(s_i,a_i,s_i',r_i)$\n",
    "\n",
    "`muestra`: genera una muestra de tamaño No_gupo\n",
    "\n",
    "`len`: función que regresa la cantidad de muestras en el repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transicion = namedtuple('Transicion',\n",
    "                        ('estado', 'accion', 'sig_estado', 'recompensa'))\n",
    "\n",
    "\n",
    "class repositorioExperiencia(object):\n",
    "\n",
    "    def __init__(self, capacidad):\n",
    "        self.capacidad = capacidad\n",
    "        self.memoria = []\n",
    "        self.posicion = 0\n",
    "\n",
    "    def guarda(self, *args):\n",
    "        \"\"\"Guarda una transición.\"\"\"\n",
    "        if len(self.memoria) < self.capacidad:\n",
    "            self.memoria.append(None)\n",
    "        self.memoria[self.posicion] = Transicion(*args)\n",
    "        self.posicion = (self.posicion + 1) % self.capacidad\n",
    "\n",
    "    def muestra(self, batch_size):\n",
    "        return random.sample(self.memoria, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda definimos una función llamda `actualiza_q` que implemente DQL:\n",
    "    \n",
    "1. Saque una muestra de tamaño `No_grupo`, \n",
    "2. Usando la `red_estrategia`, calcule $Q_{\\theta}(s_t,a_t)$ para la muestra\n",
    "3. Calcula $V^*(s_{t+1})$ usando la `red_etiqueta`\n",
    "4. Calcular la etiquetas $y_j=r_i+\\max_aQ_{\\theta'}(s_t,a)$\n",
    "5. Calcula función de pérdida para $Q_{\\theta}(s_t,a_t)-y_j$\n",
    "6. Actualize $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualiza_q():\n",
    "    if len(memoria) < No_grupo:\n",
    "        return\n",
    "    transiciones = memoria.muestra(No_grupo)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    grupo = Transicion(*zip(*transiciones))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    estados_intermedios = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          grupo.sig_estado)), device=device, dtype=torch.uint8)\n",
    "    sig_estados_intermedios = torch.cat([s for s in grupo.sig_estado\n",
    "                                                if s is not None])\n",
    "    grupo_estado = torch.cat(grupo.estado)\n",
    "    accion_grupo = torch.cat(grupo.accion)\n",
    "    recompensa_grupo = torch.cat(grupo.recompensa)\n",
    "\n",
    "    # Calcula Q(s_t, a_t) - una manera es usar la red_estrategia para calcular Q(s_t), \n",
    "    # y seleccionar las columnas usando los índices de la acciones tomadas usando la función gather\n",
    "    q_actual = \n",
    "\n",
    "    # Calcula V*(s_{t+1}) para todos los sig_estados en el grupo usando la red_etiqueta\n",
    "    valores_sig_estado = torch.zeros(No_grupo, device=device)\n",
    "    # Ten cuidado de solo calcular para los valores intermedios!\n",
    "    valores_sig_estado\n",
    "    # Calcular las etiquetas\n",
    "    y_j = \n",
    "\n",
    "    # Calcula función de pérdida de MSE\n",
    "    perdida = \n",
    "\n",
    "    # Optimizar el modelo\n",
    "    optimizador.zero_grad()\n",
    "    perdida.backward()\n",
    "    for param in red_estrategia.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizador.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar la duración\n",
    "def grafica_duracion(dur):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    duracion_t = torch.tensor(duracion_episodios, dtype=torch.float)\n",
    "    plt.title('Entrenamiento...')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Duración')\n",
    "    plt.plot(duracion_t.numpy())\n",
    "    # Toma el promedio de duración d 100 episodios y los grafica\n",
    "    if len(duracion_t) >= 15:\n",
    "        media = duracion_t.unfold(0, 15, 1).mean(1).view(-1)\n",
    "        media = torch.cat((torch.zeros(14), media))\n",
    "        plt.plot(media.numpy())\n",
    "        plt.plot([200]*len(duracion_t))\n",
    "    plt.pause(dur)  # Pausa un poco para poder veer las gráficas\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa las redes y clona los parámetros de red_estrategia a red_etiqueta\n",
    "red_estrategia = red_N().to(device)\n",
    "red_etiqueta = red_N().to(device)\n",
    "red_etiqueta.load_state_dict(red_estrategia.state_dict())\n",
    "red_etiqueta.eval()\n",
    "# Inicializa el optimizador y la memoria, juega con el learning rate y otros tipos de optimizadores:\n",
    "optimizador = optim.Adam(red_estrategia.parameters(),lr=lr)\n",
    "memoria = repositorioExperiencia(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "duracion_episodios = []\n",
    "\n",
    "# Entrenamiento\n",
    "for episodio in range(0, No_episodios):\n",
    "    # Reset the enviroment\n",
    "    estado = enviroment.reset()\n",
    "    estado = torch.tensor(estado, dtype = torch.float)\n",
    "\n",
    "    # Inicializa variables de recompensa acumulada y termina\n",
    "    \n",
    "    for t in count():\n",
    "        # Decide acción a tomar\n",
    "\n",
    "        # Implementa la acción y recibe reacción del ambiente\n",
    "\n",
    "        # Convierte a observaciones a tensores\n",
    "\n",
    "        # Si acabó (Termina = True) el episodio la recompensa es negativa\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Guarda la transición en la memoria\n",
    "        \n",
    "\n",
    "        # Actualiza valor q en la red de medida\n",
    "        \n",
    "        ## Moverse al siguiente estado\n",
    "        \n",
    "        # Si termina añade la cantidad de episodios a la lista y sale del for loop\n",
    "        if termina:\n",
    "\n",
    "    # Actualizar la red_etiqueta\n",
    "    if episodio % actualizar_red_med == 0:\n",
    "        red_etiqueta.load_state_dict(red_estrategia.state_dict())\n",
    "        # Grafica la duración de los episodios\n",
    "        grafica_duracion(0.3)\n",
    "print(\"**********************************\")\n",
    "print(\"Entrenamiento finalizado!\\n\")\n",
    "print(\"**********************************\")\n",
    "grafica_duracion(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
